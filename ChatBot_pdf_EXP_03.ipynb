{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***ChatBot_pdf_EXP 03 ***\n",
        "\n",
        "Submitted By - Prabhat Singh"
      ],
      "metadata": {
        "id": "4PWtq-n1bcZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install openai faiss-cpu pydantic PyPDF2 tiktoken\n",
        "\n",
        "import os\n",
        "import openai\n",
        "import faiss\n",
        "import numpy as np\n",
        "import PyPDF2\n",
        "import tiktoken\n",
        "from typing import List, Tuple, Dict\n",
        "from pydantic import BaseModel\n",
        "from dataclasses import dataclass\n",
        "from IPython.display import display, Markdown\n",
        "from google.colab import files\n",
        "\n",
        "# === STEP 1: Setup API Key ===\n",
        "openai.api_key = input(\"Enter your OpenAI API key: \")\n",
        "\n",
        "# === STEP 2: Upload PDF ===\n",
        "print(\"📂 Please upload the PDF file of 'The Hard Thing About Hard Things'\")\n",
        "uploaded = files.upload()\n",
        "pdf_path = list(uploaded.keys())[0]\n",
        "\n",
        "# === STEP 3: Message Schema for Memory ===\n",
        "class Message(BaseModel):\n",
        "    role: str\n",
        "    content: str\n",
        "\n",
        "# === STEP 4: PDF Chunking ===\n",
        "def load_pdf_chunks(pdf_path: str, max_tokens: int = 300) -> List[Dict]:\n",
        "    reader = PyPDF2.PdfReader(pdf_path)\n",
        "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    chunks = []\n",
        "    for i, page in enumerate(reader.pages):\n",
        "        text = page.extract_text()\n",
        "        if not text:\n",
        "            continue\n",
        "        tokens = tokenizer.encode(text)\n",
        "        start = 0\n",
        "        while start < len(tokens):\n",
        "            end = min(start + max_tokens, len(tokens))\n",
        "            chunk_text = tokenizer.decode(tokens[start:end])\n",
        "            chunks.append({\"text\": chunk_text, \"page\": i + 1})\n",
        "            start = end\n",
        "    return chunks\n",
        "\n",
        "# === STEP 5: Embedding Generator ===\n",
        "class Embedder:\n",
        "    def __init__(self, model: str = \"text-embedding-3-small\"):\n",
        "        self.model = model\n",
        "\n",
        "    def get_embedding(self, text: str) -> List[float]:\n",
        "        response = openai.embeddings.create(input=[text], model=self.model)\n",
        "        return response.data[0].embedding\n",
        "\n",
        "def get_memory_conditioned_embedding(messages: List[Message], embedder: Embedder) -> List[float]:\n",
        "    prompt_context = \"\\n\".join([f\"{msg.role}: {msg.content}\" for msg in messages])\n",
        "    return embedder.get_embedding(prompt_context)\n",
        "\n",
        "# === STEP 6: FAISS Vector Store ===\n",
        "@dataclass\n",
        "class DocumentChunk:\n",
        "    text: str\n",
        "    page: int\n",
        "    embedding: List[float]\n",
        "\n",
        "class LocalVectorStore:\n",
        "    def __init__(self, dim: int):\n",
        "        self.index = faiss.IndexFlatL2(dim)\n",
        "        self.chunks: List[DocumentChunk] = []\n",
        "\n",
        "    def add(self, chunk: DocumentChunk):\n",
        "        self.index.add(np.array([chunk.embedding], dtype=np.float32))\n",
        "        self.chunks.append(chunk)\n",
        "\n",
        "    def search(self, query_embedding: List[float], k: int = 5) -> List[Tuple[str, int]]:\n",
        "        D, I = self.index.search(np.array([query_embedding], dtype=np.float32), k)\n",
        "        return [(self.chunks[i].text, self.chunks[i].page) for i in I[0]]\n",
        "\n",
        "# === STEP 7: Chatbot with Memory-Aware Embedding ===\n",
        "class RAGChatbot:\n",
        "    def __init__(self, embedder: Embedder, store: LocalVectorStore):\n",
        "        self.embedder = embedder\n",
        "        self.store = store\n",
        "        self.messages: List[Message] = []\n",
        "\n",
        "    def ask(self, user_input: str) -> str:\n",
        "        self.messages.append(Message(role=\"user\", content=user_input))\n",
        "        query_embedding = get_memory_conditioned_embedding(self.messages, self.embedder)\n",
        "        retrieved = self.store.search(query_embedding, k=5)\n",
        "\n",
        "        context = \"\\n\\n\".join([f\"(Page {page}) {text}\" for text, page in retrieved])\n",
        "        conversation = \"\\n\".join([f\"{msg.role}: {msg.content}\" for msg in self.messages])\n",
        "\n",
        "        prompt = f\"\"\"You are a helpful assistant discussing the book 'The Hard Thing About Hard Things' by Ben Horowitz.\n",
        "Use the context below to answer the user's question. Always cite relevant quotes and page numbers.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Conversation so far:\n",
        "{conversation}\n",
        "\n",
        "assistant:\"\"\"\n",
        "\n",
        "        response = openai.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\"role\": \"system\", \"content\": prompt}],\n",
        "            temperature=0.2\n",
        "        )\n",
        "        answer = response.choices[0].message.content\n",
        "        self.messages.append(Message(role=\"assistant\", content=answer))\n",
        "        return answer\n",
        "\n",
        "# === STEP 8: Process PDF and Launch Chat ===\n",
        "print(\"📄 Chunking PDF...\")\n",
        "raw_chunks = load_pdf_chunks(pdf_path)\n",
        "\n",
        "print(\"🔎 Generating embeddings and building FAISS index...\")\n",
        "embedder = Embedder()\n",
        "dim = len(embedder.get_embedding(\"test\"))\n",
        "store = LocalVectorStore(dim=dim)\n",
        "\n",
        "for chunk in raw_chunks:\n",
        "    embedding = embedder.get_embedding(chunk[\"text\"])\n",
        "    store.add(DocumentChunk(text=chunk[\"text\"], page=chunk[\"page\"], embedding=embedding))\n",
        "\n",
        "bot = RAGChatbot(embedder, store)\n",
        "\n",
        "# === STEP 9: Chat Interface ===\n",
        "print(\"\\n✅ Ready! Ask questions about the book (type 'exit' to stop):\")\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.strip().lower() in {\"exit\", \"quit\"}:\n",
        "        break\n",
        "    answer = bot.ask(user_input)\n",
        "    display(Markdown(f\"**Assistant:** {answer}\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OVj0ieQ3MPro",
        "outputId": "f4dedc95-562d-4539-9b25-794a39d4d230"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.78.0)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (2.11.4)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.4.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
            "Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2, faiss-cpu\n",
            "Successfully installed PyPDF2-3.0.1 faiss-cpu-1.11.0\n",
            "Enter your OpenAI API key: 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\n",
            "📂 Please upload the PDF file of 'The Hard Thing About Hard Things'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-89e96bcc-0fb9-4792-8b91-93e7008a5819\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-89e96bcc-0fb9-4792-8b91-93e7008a5819\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving the_hard_thing_about_hard_things.pdf to the_hard_thing_about_hard_things.pdf\n",
            "📄 Chunking PDF...\n",
            "🔎 Generating embeddings and building FAISS index...\n",
            "\n",
            "✅ Ready! Ask questions about the book (type 'exit' to stop):\n",
            "You: what is pdf about\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Assistant:** The book \"The Hard Thing About Hard Things\" by Ben Horowitz is about the challenges and difficulties of running a business, particularly in the technology sector. It provides insights and advice on how to handle the tough decisions and situations that arise when leading a company. The book covers topics such as managing and training employees, making strategic decisions, and dealing with competition. It emphasizes the importance of good product management and effective communication within a company. For example, it discusses the qualities of good versus bad product managers and the significance of training programs (pages 96-97)."
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: give me the all chapter names\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Assistant:** Here are the chapter names from \"The Hard Thing About Hard Things\" by Ben Horowitz:\n\n1. From Communist to Venture Capitalist\n2. “I Will Survive”\n3. This Time with Feeling\n4. When Things Fall Apart\n   - The Struggle\n   - CEOs Should Tell It Like It Is\n   - The Right Way to Lay People Off\n   - Preparing to Fire an Executive\n   - Demoting a Loyal Friend\n   - Lies That Losers Tell\n   - Lead Bullets\n   - Nobody Cares\n5. Take Care of the People, the Products, and the Profits—in That Order\n   - A Good Place to Work\n   - Why Startups Should Train Their People\n   - Is It Okay to Hire People from Your Friend’s Company?\n   - Why It’s Hard to Bring Big Company Execs into Little Companies\n   - Hiring Executives: If You’ve Never Done the Job, How Do You Hire Somebody Good?\n   - When Employees Misinterpret Managers\n   - Management Debt\n   - Management Quality Assurance\n6. Concerning the Going Concern\n   - How to Minimize Politics in\n7. How to Lead Even When You Don’t Know Where You Are Going\n   - The Most Difficult CEO Skill\n   - The Fine Line Between Fear and Courage\n   - Ones and Twos\n   - Follow the Leader\n   - Peacetime CEO/Wartime CEO\n   - Making Yourself a CEO\n   - How to Evaluate CEOs\n8. First Rule of Entrepreneurship: There Are No Rules\n   - Solving the Accountability vs. Creativity Paradox\n   - The Freaky Friday Management Technique\n   - Staying Great\n   - Should You Sell Your Company?\n9. The End of the Beginning\n\nAdditionally, there is an appendix titled \"Questions for Head of Enterprise Sales Force.\""
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: quit\n"
          ]
        }
      ]
    }
  ]
}